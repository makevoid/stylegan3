ARG NVIDIA_BUILD_REF
LABEL com.nvidia.build.id=28019337
ENV NVIDIA_BUILD_ID=28019337
ARG NVIDIA_BUILD_ID
COPY entrypoint.d/ /opt/nvidia/entrypoint.d/ # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c ln -sf ${_CUDA_COMPAT_PATH}/lib.real ${_CUDA_COMPAT_PATH}/lib && echo ${_CUDA_COMPAT_PATH}/lib > /etc/ld.so.conf.d/00-cuda-compat.conf && ldconfig && rm -f ${_CUDA_COMPAT_PATH}/lib # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c mkdir -p /opt/dlprof && mkdir -p /opt/dlprof_viewer_install && cp /nvidia/tmp/pip/* /opt/dlprof_viewer_install/ && cp /nvidia/opt/dlprof/bin/dlprof /opt/dlprof/ && cp /nvidia/workspace/LICENSE /opt/dlprof/ && cp /nvidia/workspace/README.md /opt/dlprof/ && ln -sf /opt/dlprof/dlprof /usr/local/bin/dlprof && cd /nvidia/opt/dlprof/bin/ && pip install --no-cache-dir nvidia_dlprof_pytorch_nvtx* && cd /opt/dlprof_viewer_install && pip install --no-cache-dir nvidia_dlprofviewer-* && rm -rf /opt/conda/lib/python3.8/site-packages/qa # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c pip --version && python -c 'import sys; print(sys.platform)' && pip install --no-cache-dir nvidia-pyindex && pip install --extra-index-url https://urm.nvidia.com/artifactory/api/pypi/sw-tensorrt-pypi/simple --no-cache-dir polygraphy==0.33.0 && if [[ $TARGETARCH = "amd64" ]] ; then pip install --extra-index-url http://sqrl/dldata/pip-simple --trusted-host sqrl --no-cache-dir pytorch-quantization==2.1.0; fi # buildkit
ENV PATH=/opt/conda/bin:/opt/cmake-3.19.3-Linux-$(uname -m)/bin/:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin:/opt/tensorrt/bin
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c URL=$(VERIFY=1 /nvidia/build-scripts/installTRT.sh 2>/dev/null | sed -n "s/^.*\(http.*\)deb.*$/\1/p")tar && FILE=$(wget -O - $URL 2>/dev/null | sed -n 's/^.*href="\(TensorRT[^"]*\)".*$/\1/p' | grep -v internal) && wget --quiet $URL/$FILE -O - | tar -xz && PY=$(python -c 'import sys; print(str(sys.version_info[0])+str(sys.version_info[1]))') && pip install TensorRT-*/python/tensorrt-*-cp$PY*.whl && pip install TensorRT-*/graphsurgeon/graphsurgeon-*.whl && pip install TensorRT-*/uff/uff-*.whl && mv /usr/src/tensorrt /opt && ln -s /opt/tensorrt /usr/src/tensorrt && rm -r TensorRT-* && UFF_PATH=$(pip show uff | sed -n 's/Location: \(.*\)$/\1/p')/uff && sed -i 's/from tensorflow import GraphDef/from tensorflow.python import GraphDef/' $UFF_PATH/converters/tensorflow/conversion_helpers.py && chmod +x ${UFF_PATH}/bin/convert_to_uff.py && ln -sf ${UFF_PATH}/bin/convert_to_uff.py /usr/local/bin/convert-to-uff # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c chmod -R a+w . # buildkit
COPY tutorials tutorials # buildkit
COPY examples examples # buildkit
COPY docker-examples docker-examples # buildkit
COPY NVREADME.md README.md # buildkit
WORKDIR /workspace
ENV PYTHONIOENCODING=utf-8 LC_ALL=C.UTF-8
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c pip uninstall -y pillow && cd /tmp && git clone https://github.com/uploadcare/pillow-simd && cd pillow-simd && git fetch --all --tags --prune && git checkout tags/8.2.0 && sed -i 's/DEBUG = False/DEBUG = True/' setup.py && sed -i 's#JPEG_ROOT = None#JPEG_ROOT = "/usr/lib"#' setup.py && if [[ $TARGETARCH = "amd64" ]] ; then CC="cc -mavx" pip install --no-cache-dir --disable-pip-version-check . ; fi && if [[ $TARGETARCH = "arm64" ]] ; then pip install --no-cache-dir --disable-pip-version-check . ; fi && rm -rf ../pillow-simd # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c ( cd vision && FORCE_CUDA=1 pip install --no-cache-dir --disable-pip-version-check . ) && ( cd vision && mkdir build && cd build && cmake -DWITH_CUDA=1 -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` .. && make && make install ) && ( cd apex && pip install --no-cache-dir --disable-pip-version-check --global-option="--cpp_ext" --global-option="--cuda_ext" --global-option="--bnp" --global-option="--xentropy" --global-option="--deprecated_fused_adam" --global-option="--deprecated_fused_lamb" --global-option="--fast_multihead_attn" --global-option="--distributed_lamb" --global-option="--fast_layer_norm" --global-option="--transducer" --global-option="--distributed_adam" --global-option="--fmha" --global-option="--fast_bottleneck" . ) && ( cd text && pip install --no-cache-dir --disable-pip-version-check . ) && ( cd pytorch/third_party/onnx && pip uninstall typing -y && pip install --no-cache-dir --disable-pip-version-check . ) # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c patch -p0 < apex_mha.patch # buildkit
COPY singularity/ /.singularity.d/ # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c export COCOAPI_TAG=$(echo ${COCOAPI_VERSION} | sed 's/^.*+n//') && pip install --no-cache-dir cython pybind11 && pip install --no-cache-dir git+https://github.com/nvidia/cocoapi.git@${COCOAPI_TAG}#subdirectory=PythonAPI # buildkit
ENV COCOAPI_VERSION=2.0+nv0.5.1
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c if dpkg --compare-versions ${DALI_VERSION} lt 0.23; then DALI_URL_SUFFIX="/cuda/${CUDA_VERSION%%.*}.0"; else DALI_PKG_SUFFIX="-cuda${CUDA_VERSION%%.*}0"; fi && pip install --no-cache-dir --extra-index-url https://developer.download.nvidia.com/compute/redist --extra-index-url http://sqrl/dldata/pip-dali${DALI_URL_SUFFIX:-} --trusted-host sqrl nvidia-dali${DALI_PKG_SUFFIX:-}==${DALI_VERSION} # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c cd pytorch && USE_CUPTI_SO=1 USE_KINETO=1 CMAKE_PREFIX_PATH="$(dirname $(which conda))/../" NCCL_INCLUDE_DIR="/usr/include/" NCCL_LIB_DIR="/usr/lib/" USE_SYSTEM_NCCL=1 CFLAGS=$PYT_CFLAGS python setup.py install && python setup.py clean # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c if [[ $TARGETARCH = "amd64" ]]; then export PYT_CFLAGS="-fno-gnu-unique"; fi && if [[ $TARGETARCH = "arm64" ]]; then export PYT_CFLAGS="-fno-gnu-unique -fsigned-char"; fi # buildkit
ENV PYT_CFLAGS=-fno-gnu-unique
ENV CUDA_HOME=/usr/local/cuda
ENV TORCH_CUDA_ARCH_LIST=5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c pip install --no-cache-dir expecttest # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c pip install --no-cache-dir librosa # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c pip install --no-cache-dir typing_extensions # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c conda build magma-cuda/ && conda install `conda build --output magma-cuda` && rm -rf conda `conda build --output magma-cuda` && /opt/conda/bin/conda clean -ya # buildkit
EXPOSE map[6006/tcp:{}]
EXPOSE map[8888/tcp:{}]
ENV TENSORBOARD_PORT=6006
ENV JUPYTER_PORT=8888
COPY jupyter_notebook_config.py /opt/conda/etc/jupyter/ # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c pip install --no-cache-dir git+https://github.com/cliffwoolley/jupyter_tensorboard.git@0.2.0+nv21.03 && mkdir -p $NVM_DIR && curl -Lo- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash && source "$NVM_DIR/nvm.sh" && nvm install 16.6.1 node && jupyter labextension install jupyterlab_tensorboard && jupyter serverextension enable jupyterlab && pip install --no-cache-dir jupytext && jupyter labextension install jupyterlab-jupytext@1.2.2 && ( cd $NVM_DIR/versions/node/$(node -v)/lib/node_modules/npm && npm install tar@^6.1.1 --production ) && ( cd $NVM_DIR/versions/node/$(node -v)/lib/node_modules/npm && npm prune --production ) && ( cd /opt/conda/share/jupyter/lab/staging/node_modules/@jupyterlab/coreutils && npm install url-parse@^1.5.0 --production ) && ( cd /opt/conda/share/jupyter/lab/staging && npm prune --production ) && npm cache clean --force && rm -rf /usr/local/share/.cache && echo "source $NVM_DIR/nvm.sh" >> /etc/bash.bashrc && mv /root/.jupyter/jupyter_notebook_config.json /opt/conda/etc/jupyter/ # buildkit
ENV NVM_DIR=/usr/local/nvm
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c PATCHED_FILE=$(python -c "from tensorboard.plugins.core import core_plugin as _; print(_.__file__)") && sed -i 's/^\( *"--bind_all",\)$/\1 default=True,/' "$PATCHED_FILE" && test $(grep '^ *"--bind_all", default=True,$' "$PATCHED_FILE" | wc -l) -eq 1 # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c pip install --no-cache-dir tensorboard # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c pip install --no-cache-dir 'nltk>=3.6.4' && pip install --no-cache-dir -r text/requirements.txt && pip install --no-cache-dir -r caffe2_requirements.txt && pip install --no-cache-dir notebook==6.4.1 jupyterlab==2.3.2 python-hostlist # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c JPEG_TURBO_VERSION=1.5.3 && wget -q -O - https://github.com/libjpeg-turbo/libjpeg-turbo/archive/${JPEG_TURBO_VERSION}.tar.gz | tar -xzf - && cd libjpeg-turbo-${JPEG_TURBO_VERSION} && autoreconf -fiv && ./configure --enable-shared --prefix=/usr 2>&1 >/dev/null && make -j"$(nproc)" install 2>&1 >/dev/null && rm -rf /libjpeg-turbo-${JPEG_TURBO_VERSION} # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c OPENCV_VERSION=3.4.11 && cd / && wget -q -O - https://github.com/opencv/opencv/archive/${OPENCV_VERSION}.tar.gz | tar -xzf - && cd /opencv-${OPENCV_VERSION} && mkdir build && cd build && cmake -DCMAKE_BUILD_TYPE=RELEASE -DCMAKE_INSTALL_PREFIX=/usr -DWITH_CUDA=OFF -DWITH_1394=OFF -DPYTHON3_PACKAGES_PATH=$(python -c "from distutils.sysconfig import *; print(get_python_lib())") -DBUILD_opencv_cudalegacy=OFF -DBUILD_opencv_stitching=OFF -DWITH_IPP=OFF -DWITH_PROTOBUF=OFF .. && make -j"$(nproc)" install && rm -rf /opencv-${OPENCV_VERSION} # buildkit
COPY . . # buildkit
WORKDIR /opt/pytorch
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c pip install tqdm --upgrade # buildkit
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c pip install hypothesis==4.50.8 # buildkit
ENV PATH=/opt/conda/bin:/opt/cmake-3.19.3-Linux-$(uname -m)/bin/:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin
RUN |4 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 PYVER=3.8 /bin/sh -c wget -O ~/miniforge.sh https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-$(uname -m).sh && chmod +x ~/miniforge.sh && ~/miniforge.sh -b -p /opt/conda && rm ~/miniforge.sh && /opt/conda/bin/conda install -y python=$PYVER cmake conda-build numpy pyyaml scipy==1.6.3 ipython ninja spacy mock numba==0.52.0 openblas!=0.3.6 && if [[ $TARGETARCH = "amd64" ]] ; then /opt/conda/bin/conda install -y mkl=2019.5 mkl-include=2019.5 ; fi && /opt/conda/bin/conda clean -ya # buildkit
ARG PYVER=3.8
ENV PATH=/opt/cmake-3.19.3-Linux-$(uname -m)/bin/:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin
RUN |3 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 /bin/sh -c curl -L -k -o /opt/cmake-3.19.3-Linux-$(uname -m).tar.gz https://github.com/Kitware/CMake/releases/download/v3.19.3/cmake-3.19.3-Linux-$(uname -m).tar.gz && pushd /opt && tar -xzf cmake-3.19.3-Linux-$(uname -m).tar.gz && rm cmake-3.19.3-Linux-$(uname -m).tar.gz && popd # buildkit
RUN |3 NVIDIA_PYTORCH_VERSION=21.10 PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c TARGETARCH=amd64 /bin/sh -c apt-get update && apt-get install -y --no-install-recommends autoconf automake libatlas-base-dev libboost-program-options-dev libgoogle-glog-dev libbz2-dev libleveldb-dev liblmdb-dev libprotobuf-dev libsnappy-dev libtool libjsoncpp-dev nasm protobuf-compiler pkg-config unzip sox libsndfile1 libpng-dev libhdf5-103 libhdf5-dev && rm -rf /var/lib/apt/lists/* # buildkit
ARG TARGETARCH
LABEL com.nvidia.pytorch.version=1.10.0a0+0aef44c
ENV PYTORCH_BUILD_VERSION=1.10.0a0+0aef44c PYTORCH_VERSION=1.10.0a0+0aef44c PYTORCH_BUILD_NUMBER=0 NVIDIA_PYTORCH_VERSION=21.10
ARG PYTORCH_BUILD_VERSION
ARG NVIDIA_PYTORCH_VERSION
ENV NVIDIA_PRODUCT_NAME=PyTorch
RUN |7 GDRCOPY_VERSION=2.3 HPCX_VERSION=2.9.0 RDMACORE_VERSION=36.0 MOFED_VERSION=5.4-rdmacore36.0 OPENUCX_VERSION=1.11.0-rc1 OPENMPI_VERSION=4.1.1 TARGETARCH=amd64 /bin/sh -c if [[ "$CUDA_VERSION" == "11.2.1.007" && $(dpkg --print-architecture) == "amd64" ]]; then wget http://sqrl.nvidia.com/dldata/sgodithi/bug3254800/cicc >/dev/null 2>&1 && cp cicc /usr/local/cuda/nvvm/bin/. ; fi # buildkit
ENV LIBRARY_PATH=/usr/local/cuda/lib64/stubs:
RUN |7 GDRCOPY_VERSION=2.3 HPCX_VERSION=2.9.0 RDMACORE_VERSION=36.0 MOFED_VERSION=5.4-rdmacore36.0 OPENUCX_VERSION=1.11.0-rc1 OPENMPI_VERSION=4.1.1 TARGETARCH=amd64 /bin/sh -c export DEVEL=1 BASE=0 && /nvidia/build-scripts/installNCU.sh && /nvidia/build-scripts/installCUDA.sh && /nvidia/build-scripts/installLIBS.sh && /nvidia/build-scripts/installNCCL.sh && /nvidia/build-scripts/installCUDNN.sh && /nvidia/build-scripts/installCUTENSOR.sh && /nvidia/build-scripts/installTRT.sh && /nvidia/build-scripts/installNSYS.sh && if [ -f "/tmp/cuda-${_CUDA_VERSION_MAJMIN}.patch" ]; then patch -p0 < /tmp/cuda-${_CUDA_VERSION_MAJMIN}.patch; fi && rm -f /tmp/cuda-*.patch # buildkit
COPY cuda-*.patch /tmp # buildkit
ENV OMPI_MCA_coll_hcoll_enable=0
ENV OPAL_PREFIX=/opt/hpcx/ompi PATH=/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin
RUN |7 GDRCOPY_VERSION=2.3 HPCX_VERSION=2.9.0 RDMACORE_VERSION=36.0 MOFED_VERSION=5.4-rdmacore36.0 OPENUCX_VERSION=1.11.0-rc1 OPENMPI_VERSION=4.1.1 TARGETARCH=amd64 /bin/sh -c cd /nvidia && ( cd opt/rdma-core/ && dpkg -i libibverbs1_*.deb libibverbs-dev_*.deb librdmacm1_*.deb librdmacm-dev_*.deb libibumad3_*.deb libibumad-dev_*.deb ibverbs-utils_*.deb ibverbs-providers_*.deb && rm $(dpkg-query -L libibverbs-dev librdmacm-dev libibumad-dev | grep "\(\.so\|\.a\)$") ) && ( cd opt/gdrcopy/ && dpkg -i libgdrapi_*.deb ) && ( cp -r opt/hpcx /opt/ && cp etc/ld.so.conf.d/hpcx.conf /etc/ld.so.conf.d/ && ln -sf /opt/hpcx/ompi /usr/local/mpi && ln -sf /opt/hpcx/ucx /usr/local/ucx && sed -i 's/^\(hwloc_base_binding_policy\) = core$/\1 = none/' /opt/hpcx/ompi/etc/openmpi-mca-params.conf && sed -i 's/^\(btl = self\)$/#\1/' /opt/hpcx/ompi/etc/openmpi-mca-params.conf ) && ldconfig # buildkit
ARG TARGETARCH
ENV GDRCOPY_VERSION=2.3 HPCX_VERSION=2.9.0 MOFED_VERSION=5.4-rdmacore36.0 OPENUCX_VERSION=1.11.0-rc1 OPENMPI_VERSION=4.1.1 RDMACORE_VERSION=36.0
ARG OPENMPI_VERSION
ARG OPENUCX_VERSION
ARG MOFED_VERSION=5.4-rdmacore36.0
ARG RDMACORE_VERSION
ARG HPCX_VERSION
ARG GDRCOPY_VERSION
RUN /bin/sh -c export DEBIAN_FRONTEND=noninteractive && apt-get update && apt-get install -y --no-install-recommends build-essential git libglib2.0-0 less libnl-route-3-200 libnl-3-dev libnl-route-3-dev libnuma-dev libnuma1 libpmi2-0-dev nano numactl openssh-client vim wget && rm -rf /var/lib/apt/lists/* # buildkit
COPY NVIDIA_Deep_Learning_Container_License.pdf /workspace/ # buildkit
ENTRYPOINT ["/opt/nvidia/nvidia_entrypoint.sh"]
ENV NVIDIA_PRODUCT_NAME=CUDA
COPY entrypoint/ /opt/nvidia/ # buildkit
COPY /usr/local/bin/checkSMVER.sh /usr/local/bin/checkSMVER.sh # buildkit
COPY /usr/local/bin/deviceQuery /usr/local/bin/deviceQuery # buildkit
ENV PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin LD_LIBRARY_PATH=/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 NVIDIA_VISIBLE_DEVICES=all NVIDIA_DRIVER_CAPABILITIES=compute,utility,video
RUN |19 CUDA_VERSION=11.4.3.001 CUDA_DRIVER_VERSION=470.57.02 NCCL_VERSION=2.11.4 CUBLAS_VERSION=11.6.5.2 CUFFT_VERSION=10.5.2.100 CURAND_VERSION=10.2.5.120 CUSPARSE_VERSION=11.6.0.120 CUSOLVER_VERSION=11.2.0.120 CUTENSOR_VERSION=1.3.2.3 NPP_VERSION=11.4.0.110 NVJPEG_VERSION=11.5.2.120 CUDNN_VERSION=8.2.4.15 TRT_VERSION=8.0.3.4+cuda11.3.1.005 TRTOSS_VERSION=21.10 NSIGHT_SYSTEMS_VERSION=2021.3.2.4 NSIGHT_COMPUTE_VERSION=2021.2.2.1 DALI_VERSION=1.6.0 DALI_BUILD=2993096 DLPROF_VERSION=21.10 /bin/sh -c echo "/usr/local/nvidia/lib" >> /etc/ld.so.conf.d/nvidia.conf && echo "/usr/local/nvidia/lib64" >> /etc/ld.so.conf.d/nvidia.conf # buildkit
ADD docs.tgz / # buildkit
ENV DALI_VERSION=1.6.0 DALI_BUILD=2993096 DLPROF_VERSION=21.10
ARG DLPROF_VERSION
ARG DALI_BUILD
ARG DALI_VERSION
LABEL com.nvidia.nccl.version=2.11.4 com.nvidia.cublas.version=11.6.5.2 com.nvidia.cufft.version=10.5.2.100 com.nvidia.curand.version=10.2.5.120 com.nvidia.cusparse.version=11.6.0.120 com.nvidia.cusolver.version=11.2.0.120 com.nvidia.cutensor.version=1.3.2.3 com.nvidia.npp.version=11.4.0.110 com.nvidia.nvjpeg.version=11.5.2.120 com.nvidia.cudnn.version=8.2.4.15 com.nvidia.tensorrt.version=8.0.3.4+cuda11.3.1.005 com.nvidia.tensorrtoss.version=21.10 com.nvidia.nsightsystems.version=2021.3.2.4 com.nvidia.nsightcompute.version=2021.2.2.1
RUN |16 CUDA_VERSION=11.4.3.001 CUDA_DRIVER_VERSION=470.57.02 NCCL_VERSION=2.11.4 CUBLAS_VERSION=11.6.5.2 CUFFT_VERSION=10.5.2.100 CURAND_VERSION=10.2.5.120 CUSPARSE_VERSION=11.6.0.120 CUSOLVER_VERSION=11.2.0.120 CUTENSOR_VERSION=1.3.2.3 NPP_VERSION=11.4.0.110 NVJPEG_VERSION=11.5.2.120 CUDNN_VERSION=8.2.4.15 TRT_VERSION=8.0.3.4+cuda11.3.1.005 TRTOSS_VERSION=21.10 NSIGHT_SYSTEMS_VERSION=2021.3.2.4 NSIGHT_COMPUTE_VERSION=2021.2.2.1 /bin/sh -c /nvidia/build-scripts/installNCCL.sh && /nvidia/build-scripts/installLIBS.sh && /nvidia/build-scripts/installCUDNN.sh && /nvidia/build-scripts/installTRT.sh && /nvidia/build-scripts/installNSYS.sh && /nvidia/build-scripts/installNCU.sh && /nvidia/build-scripts/installCUTENSOR.sh # buildkit
ENV NCCL_VERSION=2.11.4 CUBLAS_VERSION=11.6.5.2 CUFFT_VERSION=10.5.2.100 CURAND_VERSION=10.2.5.120 CUSPARSE_VERSION=11.6.0.120 CUSOLVER_VERSION=11.2.0.120 CUTENSOR_VERSION=1.3.2.3 NPP_VERSION=11.4.0.110 NVJPEG_VERSION=11.5.2.120 CUDNN_VERSION=8.2.4.15 TRT_VERSION=8.0.3.4+cuda11.3.1.005 TRTOSS_VERSION=21.10 NSIGHT_SYSTEMS_VERSION=2021.3.2.4 NSIGHT_COMPUTE_VERSION=2021.2.2.1
ARG NSIGHT_COMPUTE_VERSION
ARG NSIGHT_SYSTEMS_VERSION
ARG TRTOSS_VERSION
ARG TRT_VERSION
ARG CUDNN_VERSION
ARG NVJPEG_VERSION
ARG NPP_VERSION
ARG CUTENSOR_VERSION
ARG CUSOLVER_VERSION
ARG CUSPARSE_VERSION
ARG CURAND_VERSION
ARG CUFFT_VERSION
ARG CUBLAS_VERSION
ARG NCCL_VERSION
LABEL com.nvidia.volumes.needed=nvidia_driver com.nvidia.cuda.version=9.0
ENV _CUDA_COMPAT_PATH=/usr/local/cuda/compat ENV=/etc/shinit_v2 BASH_ENV=/etc/bash.bashrc NVIDIA_REQUIRE_CUDA=cuda>=9.0
RUN |2 CUDA_VERSION=11.4.3.001 CUDA_DRIVER_VERSION=470.57.02 /bin/sh -c patch -p0 < /tmp/startup_scripts.patch && rm -f /tmp/startup_scripts.patch && ln -sf /.singularity.d/libs /usr/local/lib/singularity && ln -sf sh-wrap /bin/sh # buildkit
COPY singularity /.singularity.d # buildkit
COPY cudaCheck/startup_scripts.patch /tmp # buildkit
COPY cudaCheck/shinit_v2 /etc/shinit_v2 # buildkit
RUN |2 CUDA_VERSION=11.4.3.001 CUDA_DRIVER_VERSION=470.57.02 /bin/sh -c rm -rf /tmp/cudaCheck # buildkit
RUN |2 CUDA_VERSION=11.4.3.001 CUDA_DRIVER_VERSION=470.57.02 /bin/sh -c cp /tmp/cudaCheck/$(uname -m)/cudaCheck /usr/local/bin/ # buildkit
RUN |2 CUDA_VERSION=11.4.3.001 CUDA_DRIVER_VERSION=470.57.02 /bin/sh -c cp /tmp/cudaCheck/$(uname -m)/sh-wrap /bin/sh-wrap # buildkit
COPY cudaCheck /tmp/cudaCheck # buildkit
RUN |2 CUDA_VERSION=11.4.3.001 CUDA_DRIVER_VERSION=470.57.02 /bin/sh -c /nvidia/build-scripts/installCUDA.sh # buildkit
ENV CUDA_VERSION=11.4.3.001 CUDA_DRIVER_VERSION=470.57.02 CUDA_CACHE_DISABLE=1
ARG CUDA_DRIVER_VERSION
ARG CUDA_VERSION
RUN /bin/sh -c export DEBIAN_FRONTEND=noninteractive && apt-get update && apt-get install -y --no-install-recommends apt-utils build-essential ca-certificates curl libncurses5 libncursesw5 patch wget rsync jq gnupg libtcmalloc-minimal4 && curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub | apt-key add - && echo "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64 /" > /etc/apt/sources.list.d/cuda.list && rm -rf /var/lib/apt/lists/* # buildkit
/bin/sh -c #(nop) CMD ["bash"]
/bin/sh -c #(nop) ADD file:8d2f4a45a58b3f5426c89e2ef57164824fbf0e4d17b8a90fffa0d5ff3b4e5114 in /


ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

RUN pip install pyspng==0.1.0

# only for video generation:
# RUN pip install imageio imageio-ffmpeg==0.4.4

RUN rm -rf /workspace
WORKDIR /workspace
COPY . ./

RUN (printf '#!/bin/bash\nexec \"$@\"\n' >> /entry.sh) && chmod a+x /entry.sh
ENTRYPOINT ["/entry.sh"]
